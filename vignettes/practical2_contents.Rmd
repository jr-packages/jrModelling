## Question 1 - Simple Linear Regression

Load the relevant packages.

```{r, echo = TRUE}
library("jrModelling")
library("broom")
library("tidyverse")
```

\newthought{Consider the} data for ice cream sales at Luigi Minchella's ice cream parlour.

The data can be obtained from:
```{r, echo = TRUE}
data(icecream, package = "jrModelling")
```

1. Perform a linear regression of $sales$ on $temperature$. Should temperature be included in the model?
    ```{r, }
    m = lm(sales ~ temperature, data = icecream)
    tidy_m = tidy(m)
    tidy_m
    ##The p-value for the gradient is 9.9e-09
    ##This suggests temperature is useful
    ```

2. Calculate the sample correlation coefficient $r$.
    ```{r, }
    cor(icecream$temperature, icecream$sales)
    ```

3. Construct a graph of the data. You can use **ggplot2** or base, but as in the notes we'll be using **ggplot2** for the solutions. Add a dashed red line indicating the line of best fit.
    ```{r,F1, fig.keep='none', message = FALSE, warning = FALSE}
    library("ggplot2")
    ggplot(icecream, aes(x = temperature, y = sales)) +
        geom_point() +
        geom_abline(intercept = tidy_m$estimate[1],
                        slope = tidy_m$estimate[2],
                    linetype = 2, colour = "red") + 
        labs(x = "Temp", y = "Sales")
    ```

```{r,fig.margin = TRUE, fig.cap="Scatterplot with the earnings data. Also shows the line of best fit.", out.width='\\textwidth', echo=FALSE}
    ggplot(icecream, aes(x = temperature, y = sales)) +
        geom_point() +
        geom_abline(intercept = tidy_m$estimate[1],
                        slope = tidy_m$estimate[2],
                    linetype = 2, colour = "red") + 
        labs(x = "Temp", y = "Sales") + 
    annotate("label", x = 6, y = 125, label = "r = 0.983") + 
    theme_bw()
```

4. Using the `annotate()` function, add the text *r = 0.983* to your plot. See `?annotate` for more details or ask your presenter!
    ```{r, fig.keep='none', eval = FALSE}
    r = round(cor(icecream$temperature, icecream$sales), 2)
    ggplot(icecream, aes(x = temperature, y = sales)) +
        geom_point() +
        geom_abline(intercept = tidy_m$estimate[1],
                        slope = tidy_m$estimate[2],
                    linetype = 2, colour = "red") + 
        labs(x = "Temp", y = "Sales") + 
    annotate("label", x = 6, y = 125, label = paste("r =", r))
    ```

5. Plot the standardised residuals against the fitted values. Does the graph
look random? Hint: Use `augment()`
    ```{r,F2, fig.keep='none', message = FALSE, warning = FALSE}
    ##Model diagnosics look good
    m_aug = augment(m)
    ggplot(m_aug, aes(x = .fitted, y = .std.resid)) +
        geom_point() +
        geom_hline(
            yintercept = c(0, -2, 2),
            linetype = c(2, 3, 3),
            colour = c("red", "green", "green")
        )
    ```

6. Construct a q-q plot of the standardised residuals.
    ```{r,fig.keep='none', }
    ggplot(m_aug, aes(sample = .std.resid)) +
        geom_qq() +
        geom_qq_line(colour = "steelblue",
                    linetype = 2) 
    ##Model diagnosics look good
    ```

## Question 2 - Multiple Linear Regression

\newthought{The data} are from 101 consecutive patients attending a combined
thyroid-eye clinic. The patients have an endocrine disorder, Graves'
Ophthalmopathy, which affects various aspects of their eyesight. The
ophthalmologist measures various aspects of their eyesight and constructs an
overall index of how the disease affects their eyesight. This is the
Ophthalmic Index (OI) given in the dataset. The age of the patient and their
sex are also recorded. In practice, and as this is a chronic condition which
can be ameliorated but not cured, the OI would be monitored at successive
clinic visits to check on the patient's progress. However, these data are
obtained at presentation. We are interested in how OI changes with age and
gender. The data can be obtained from
```{r, echo = TRUE, message = FALSE}
library("jrModelling")
library("broom")
data(graves)
```

1. Fit the multivariate regression model predicting OI from age and gender.
    ```{r,fig.keep='none'}
    fit = lm(OI ~ age + Sex, data = graves)
    ```

2. Examine the Standardised residual plots. Is there anything that would
suggest you have a problem with your model? What do you do?
    ```{r,fig.keep='none'}
    fit_aug = augment(fit)
    ggplot(fit_aug, aes(x = age, 
                        y = .std.resid)) +
        geom_point() +
        geom_hline(yintercept = c(0, -2, 2),
                   linetype = c(2, 3, 3),
                   colour = c("red", "green", "green"))
    
    ggplot(fit_aug, aes(x = .fitted, y = .std.resid)) +
        geom_point() + 
        geom_hline(yintercept = c(0, -2, 2), 
                     linetype = c(2, 3, 3), 
                     colour = c("red", "green", "green"))
    
    ggplot(fit_aug, aes(sample = .std.resid)) +
        geom_qq() +
        geom_abline(colour = "steelblue",
                    linetype = 2)
    
    # The q-q plot shows the residuals deviating from the straight 
    # line at the tails which suggests that the normality assumption not satisfied.
    # The residuals in the age vs std resid plot appear to show a pattern. 
    # Looking at a histogram of the repsonse variable, we can see it is not normally distributed.
    # Consider transforming the response variable ,or adding a square term / interaction term to #      # your model.
    ```

## Question 3 - Multiple Linear Regression

\newthought{Dr Phil} comes to see you with his data. He believes that IQ can be
predicted by the number of years education. Dr Phil does not differentiate
between primary, secondary and tertiary education. He has four variables:

- `IQ` - the estimated IQ of the person (the response variable);
- `AgeBegin` - the age of the person when they commenced education;
- `AgeEnd` - the age of the person when they finished education;
- `TotalYears` - the total number of years a person spent in education.

The data can be obtained from:
```{r, echo = TRUE}
data(drphil, package = "jrModelling")
```

Read the data into R and fit the linear regression model:
\[
IQ = \beta_0 + \beta_1 AgeBegin + \beta_2 AgeEnd + \beta_3 TotalYears + \epsilon
\]
Explain what is wrong with this model? Suggest a possible remedy. 
```{r, }
(m = lm(IQ ~ AgeBegin + AgeEnd + TotalYears, data = drphil))
#The problem is TotalYears = AgeEnd - AgeBegin
#Solution: remove TotalYears
```


## Question 4 = One way ANOVA tables

\newthought{A pilot study} was developed to investigate whether music
influenced exam scores. Three groups of students listened to 10 minutes of
Mozart, silence or heavy metal before an IQ test. The results of the IQ test
can be obtained from:
```{r, echo = TRUE}
data(iq, package = "jrModelling")
```

1. Construct a one-way ANOVA table. Are there differences between treatment groups?
    ```{r}
    m = aov(score ~ music, data = iq)
    tidy_m = tidy(m)
    tidy_m    
    ## The p value is around 0.056.
    ## This suggests a difference may exist.
    ```

2. Check the standardised residuals of your model using `augment()` and **ggplot2**
    ```{r,F3, fig.keep='none',  }
    m_aug = augment(m)
    ggplot(m_aug, aes(x = .fitted, y = .std.resid)) +
        geom_point() + 
        geom_hline(yintercept = c(0, -2, 2), 
                     linetype = c(2, 3, 3), 
                     colour = c("red", "green", "green"))
    ## Residual plot looks OK
    ```

    ```{r,fig.margin = TRUE, out.width='\\textwidth', echo=FALSE, fig.cap = "Model diagnosics for the music data.", message = FALSE, warning = FALSE}
    ggplot(m_aug, aes(x = .fitted, y = .std.resid)) +
        geom_point() + 
        geom_hline(yintercept = c(0, -2, 2), 
                     linetype = c(2, 3, 3), 
                     colour = c("red", "green", "green")) +
        ylim(-2.5, 2.5) +
        theme_bw() +
        labs(x = "Fitted values",
             y = "Standardised Residuals")
    ```

3. Perform a multiple comparison test to determine where the difference lies.
```{r,  }
TukeyHSD(m)
## The p values indicate that the main differences can be found between the Mozart -
## Heavy Metal & Silence - Heavy Metal comparisons. Looking at the boxplot we can see
## that the iq scores for participants listening to Heavy Metal were lower than those
## listening to Mozart or silence. However, there was not much difference in
## performance between those listening to Mozart compared to those listening to
## silence.

ggplot(iq, aes(x = music, y = score)) +
    geom_boxplot()
```


\newthought{The following sections} use the results of the Olympic heptathlon
competition, Seoul, 1988. To enter the data into R, use the following commands
```{r, echo = TRUE}
data(hep, package = "jrModelling")
##Remove the athletes names and final scores.
hep_names = hep[, 1]
hep = hep[, 2:8]
```

## Question 5 - Hierarchical clustering

\newthought{Using the heptathlon} data set, carry out a clustering analysis. Try
different distance methods and clustering functions.

Try using `plot()` to create a cluster dendogram, and use `hep_names` in the label argument. 

```{r, fig.keep="none"}
plot(hclust(dist(hep)), labels = hep_names)
```

## Question 6 - Principal components analysis

1. Calculate the correlation matrix of the `hep` data set.
    ```{r, }
    ##Round to 2dp
    signif(cor(hep), 2)
    ```

2. Carry out a PCA on this data set.
    ```{r}
    ##Run principle components
    prcomp(hep)
    ```

3. Do you think you need to scale the data?
    ```{r, }
    ##Yes!. run800m dominates the loading since
    ##the scales differ
    prcomp(hep, scale = TRUE)
    ```

4. Construct a biplot of the data.
    ```{r,  fig.keep="none"}
    biplot(prcomp(hep, scale = TRUE))
    ```

## Solutions

Solutions are contained within this package:
```{r, echo = TRUE, eval = FALSE}
vignette("solutions2", package = "jrModelling")
```

